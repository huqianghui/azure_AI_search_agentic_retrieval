{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ecf8e22",
   "metadata": {},
   "source": [
    "## 1 - Introduction to classic RAG in Azure AI Search\n",
    "\n",
    "This notebook provides instructions and steps for setting up your development environment, resources, and variables. It also explains how models are used in this series, and how a search index schema is structured for RAG workloads.\n",
    "\n",
    "Steps in this notebook include:\n",
    "\n",
    "- Sign in to Azure\n",
    "- Set up the Azure resources used in the pipeline\n",
    "- Create a virtual environment\n",
    "- Install packages\n",
    "- Set variables for endpoints and models\n",
    "- Choose and deploy models for vectorization and chat\n",
    "- Review index schema considerations for classic RAG\n",
    "\n",
    "When you're finished with these steps, you are ready to set up the indexer pipeline and ingest your content.\n",
    "\n",
    "Sample data is a collection of PDF pages from the NASA's Earth Book that you load into Azure Storage and retrieve during indexing.\n",
    "\n",
    "This series assumes embedding and chat models on Azure OpenAI so that you can use the integrated vectorization capabilities of Azure AI Search. You can use a different provider but you might need custom skills or a different approach for indexing and embedding your content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5a98bd",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "You need the following Azure resources to run all of the script in this notebook.\n",
    "\n",
    "- [Azure Storage](https://learn.microsoft.com/azure/storage/common/storage-account-create), general purpose account, used for providing the PDFs.\n",
    "\n",
    "- [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource) provides the embedding and chat models.\n",
    "\n",
    "- [Azure AI Services multiservice account](https://learn.microsoft.com/azure/ai-services/multi-service-resource), in the same region as Azure AI Search, used for recognizing location entities in the Earth Book.\n",
    "\n",
    "- [Azure AI Search](https://learn.microsoft.com/azure/search/search-create-service-portal), basic tier or higher is recommended. Choose the same region as Azure OpenAI and Azure AI multiservice.\n",
    "\n",
    "Make sure Azure AI Search, Azure OpenAI, and Azure AI multiservice resources are in the same region. To meet the same-region requirement, start by reviewing the [regions for the embedding and chat models](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#model-summary-table-and-region-availability) you want to use. Once you identify a region, confirm that Azure AI Search with AI services integration is available in the [same region](https://learn.microsoft.com/azure/search/search-region-support#azure-public-regions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d5d5e",
   "metadata": {},
   "source": [
    "## Sign in to Azure\n",
    "\n",
    "You might not need this step, but if downstream connections fail with a 401 during indexer pipeline execution, it could be because you're using the wrong tenant or subscription. You can avoid this issue by signing in from the command line, explicitly setting the tenant ID and choosing the right subscription.\n",
    "\n",
    "This section assumes you have the [Azure CLI](https://learn.microsoft.com/cli/azure/authenticate-azure-cli-interactively).\n",
    "\n",
    "1. Open a command line prompt.\n",
    "\n",
    "1. Run this command to get the current tenant and subscription information: `az account show`\n",
    "\n",
    "1. If you have multiple subscriptions, specify the one that has Azure AI Search and Azure OpenAI: `az account set --subscription <PUT YOUR SUBSCRIPTION ID HERE>`\n",
    "\n",
    "1. If you have multiple tenants, you can list them: `az account tenant list`\n",
    "\n",
    "1. Sign in to Azure, specifying the tenant used for Azure AI Search and Azure OpenAI: `az login --tenant <PUT YOUR TENANT ID HERE> `\n",
    "\n",
    "You should now be logged in to Azure from your local device."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33488455",
   "metadata": {},
   "source": [
    "## Set up Azure resources using the Azure portal\n",
    "\n",
    "We recommend using the Azure portal for setting up resources.\n",
    "\n",
    "You must be a subscription **Owner** or **User Access Administrator** to create roles. If you don't have permission to create roles, you can use API keys instead. If you're using keys, you can skip the steps that enable system assigned managed identities.\n",
    "\n",
    "### Configure Azure Storage\n",
    "\n",
    "1. Download the sample PDF files from [nasa-e-book/earth_book_2019_text_pages](https://github.com/Azure-Samples/azure-search-sample-data/tree/main/nasa-e-book/earth_book_2019_text_pages).\n",
    "\n",
    "1. Sign in to the [Azure portal](https://portal.azure.com).\n",
    "\n",
    "1. On the Azure Storage left menu, select **Storage browser** > **Blob containers**, and then **Add container**.\n",
    "\n",
    "1. Name the container *nasa-ebooks-pdfs-all*.\n",
    "\n",
    "1. Upload the PDFs to the container.\n",
    "\n",
    "1. On the left menu, select **Settings** > **Identity** and turn on system assigned managed identity.\n",
    "\n",
    "### Configure Azure AI Search\n",
    "\n",
    "1. On the Azure AI Search left menu, select **Settings** > **Semantic ranker** and enable the free plan that authorizes 1,000 requests at no charge.\n",
    "\n",
    "1. On the left menu, select **Settings** > **Keys** and turn on role-based access control or \"both\".\n",
    "\n",
    "1. On the left menu, select **Settings** > **Identity** and turn on system assigned managed identity.\n",
    "\n",
    "### Configure Azure OpenAI\n",
    "\n",
    "Deploy the following models on Azure OpenAI:\n",
    "\n",
    "- text-embedding-3-large on Azure OpenAI for embeddings\n",
    "- gpt-4o on Azure OpenAI for chat completion\n",
    "\n",
    "You must have [**Cognitive Services OpenAI Contributor**](https://learn.microsoft.com/azure/ai-foundry/openai/how-to/role-based-access-control?view=foundry-classic#cognitive-services-openai-contributor) or higher to deploy models in Azure OpenAI.\n",
    "\n",
    "1. Go to [Azure OpenAI Studio](https://oai.azure.com/).\n",
    "\n",
    "1. Select **Deployments** on the left menu.\n",
    "\n",
    "1. Select **Deploy model** > **Deploy base model**.\n",
    "\n",
    "1. Select **text-embedding-3-large** from the dropdown list and confirm the selection.\n",
    "\n",
    "1. Specify a deployment name. We recommend \"text-embedding-3-large\".\n",
    "\n",
    "1. Accept the defaults.\n",
    "\n",
    "1. Select **Deploy**.\n",
    "\n",
    "1. Repeat the previous steps for **gpt-4o**.\n",
    "\n",
    "Make a note of the model names and endpoint. Embedding skills and vectorizers assemble the full endpoint internally, so you only need the resource URI. For example, given `https://MY-FAKE-ACCOUNT.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-06-01`, the endpoint you should provide in skill and vectorizer definitions is `https://MY-FAKE-ACCOUNT.openai.azure.com`.\n",
    "\n",
    "### Configure search engine role-based access to Azure Storage\n",
    "\n",
    "1. Sign in to the [Azure portal](https://portal.azure.com) and find your storage account.\n",
    "\n",
    "1. On the left menu, select **Access control (IAM)**.\n",
    "\n",
    "1. Add a role for **Storage Blob Data Reader**, assigned to the search service system-managed identity.\n",
    "\n",
    "### Configure search engine role-based access to Azure models\n",
    "\n",
    "Assign yourself *and* the search service identity permissions on Azure OpenAI. The code for this series runs locally. Requests to Azure OpenAI originate from your system. Also, embedding requests and query responses from the search engine are passed to Azure OpenAI. For these reasons, both you and the search service need permissions on Azure OpenAI.\n",
    "\n",
    "1. Sign in to the [Azure portal](https://portal.azure.com) and find your Azure OpenAI resource.\n",
    "\n",
    "1. On the left menu, select **Access control (IAM)**.\n",
    "\n",
    "1. Add a role for [**Cognitive Services OpenAI User**](https://learn.microsoft.com/azure/ai-foundry/openai/how-to/role-based-access-control?view=foundry-classic#cognitive-services-openai-contributor).\n",
    "\n",
    "1. Select **Managed identity** and then select **Members**. Find the system-managed identity for your search service in the dropdown list.\n",
    "\n",
    "1. Next, select **User, group, or service principal** and then select **Members**. Search for your user account and then select it from the dropdown list.\n",
    "\n",
    "1. Select **Review and Assign** to create the role assignments.\n",
    "\n",
    "This step concludes provisioning services in the Azure portal. Continuing to the next section, you switch to Visual Studio Code and a local environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78827e6",
   "metadata": {},
   "source": [
    "## Create a virtual environment in Visual Studio Code\n",
    "\n",
    "Create a virtual environment so that you can install the dependencies in isolation.\n",
    "\n",
    "1. In Visual Studio Code, open the folder containing 1-introduction-and-setup.ipynb.\n",
    "\n",
    "1. Press Ctrl-shift-P to open the command palette, search for \"Python: Create Environment\", and then select `Venv` to create a virtual environment in the current workspace.\n",
    "\n",
    "1. Select requirements.txt for the dependencies.\n",
    "\n",
    "It takes several minutes to create the environment. When the environment is ready, continue to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e765c6c",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df675a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3606e46b",
   "metadata": {},
   "source": [
    "## Set endpoints\n",
    "\n",
    "Provide the endpoints you collected in a previous step. You can leave the API keys empty if you enabled role-based authentication. Otherwise, if you can't use roles, provide API keys for each resource.\n",
    "\n",
    "The Azure AI multiservice account is used for skills processing. The multiservice account key must be provided, even if you're using role-based access control. The key isn't used on the connection, but it's currently used for billing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c563812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set endpoints and API keys for Azure services\n",
    "AZURE_SEARCH_SERVICE: str = \"PUT YOUR SEARCH SERVICE URL HERE\"\n",
    "# AZURE_SEARCH_KEY: str = \"DELETE IF USING ROLES, OTHERWISE PUT YOUR SEARCH SERVICE ADMIN KEY HERE\"\n",
    "AZURE_OPENAI_ACCOUNT: str = \"PUT YOUR AZURE OPENAI ACCOUNT URL HERE\"\n",
    "# AZURE_OPENAI_KEY: str = \"DELETE IF USING ROLES, OTHERWISE PUT YOUR AZURE OPENAI KEY HERE\"\n",
    "AZURE_AI_MULTISERVICE_ACCOUNT: str = \"PUT YOUR AZURE AI MULTISERVICE ACCOUNT URL HERE\"\n",
    "AZURE_AI_MULTISERVICE_KEY: str = \"PUT YOUR AZURE AI MULTISERVICE KEY HERE. ROLES ARE USED TO CONNECT. KEY IS USED FOR BILLING.\"\n",
    "AZURE_STORAGE_CONNECTION: str = \"PUT YOUR AZURE STORAGE CONNECTION STRING HERE (see example below for syntax)\"\n",
    "\n",
    "# Example connection string for a search service managed identity connection:\n",
    "# \"ResourceId=/subscriptions/FAKE-SUBCRIPTION=ID/resourceGroups/FAKE-RESOURCE-GROUP/providers/Microsoft.Storage/storageAccounts/FAKE-ACCOUNT;\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead1c46",
   "metadata": {},
   "source": [
    "## Choose models\n",
    "\n",
    "A RAG solution built on Azure AI Search takes a dependency on embedding models for vectorization, and on chat completion models for conversational search over your data.\n",
    "\n",
    "You need a model provider, such as [Azure OpenAI](https://learn.microsoft.com/azure/ai-foundry/openai/how-to/create-resource), Azure Vision in Foundry Tools via a [Microsoft Foundry resource](https://learn.microsoft.com/azure/ai-services/multi-service-resource?pivots=azportal), or the [Foundry model catalog](https://ai.azure.com/?cid=learnDocs). For Azure Vision, ensure that your Foundry resource is in the same region as [Azure AI Search](https://learn.microsoft.com/azure/search/search-region-support) and the [Azure Vision multimodal APIs](https://learn.microsoft.com/azure/ai-services/computer-vision/overview-image-analysis?tabs=4-0#region-availability).\n",
    "\n",
    "We use Azure OpenAI in this series. Other providers are listed so that you know your options for integrated vectorization.\n",
    "\n",
    "### Review models supporting built-in vectorization\n",
    "\n",
    "Vectorized content improves the query results in a RAG solution. Azure AI Search supports a built-in vectorization action in an indexing pipeline. It also supports vectorization at query time, converting text or image inputs into embeddings for a vector search. In this step, identify an embedding model that works for your content and queries. If you're providing raw vector data and raw vector queries, or if your RAG solution doesn't include vector data, skip this step.\n",
    "\n",
    "Vector queries that include a text-to-vector conversion step must use the same embedding model that was used during indexing. The search engine doesn't throw an error if you use different models, but you get poor results.\n",
    "\n",
    "To meet the same-model requirement, choose embedding models that can be referenced through *skills* during indexing and through *vectorizers* during query execution. The following table lists the skill and vectorizer pairs. To see how the embedding models are used, skip ahead to [Set up an indexing pipeline](2-build-the-pipeline.ipynb) for code that calls an embedding skill and a matching vectorizer. \n",
    "\n",
    "Azure AI Search provides skill and vectorizer support for the following embedding models in the Azure cloud.\n",
    "\n",
    "| Client | Embedding models | Skill | Vectorizer |\n",
    "|--------|------------------|-------|------------|\n",
    "| Azure OpenAI | text-embedding-ada-002<br>text-embedding-3-large<br>text-embedding-3-small | [AzureOpenAIEmbedding](https://learn.microsoft.com/azure/search/cognitive-search-skill-azure-openai-embedding) | [AzureOpenAIEmbedding](https://learn.microsoft.com/azure/search/vector-search-vectorizer-azure-open-ai) |\n",
    "| Azure Vision | multimodal 4.0 | [AzureAIVision](https://learn.microsoft.com/azure/search/cognitive-search-skill-vision-vectorize) | [AzureAIVision](https://learn.microsoft.com/azure/search/vector-search-vectorizer-ai-services-vision) |\n",
    "| Foundry model catalog | Cohere-embed-v3-english <br>Cohere-embed-v3-multilingual <br>Cohere-embed-v4 | [AML](https://learn.microsoft.com/azure/search/cognitive-search-aml-skill)  | [Foundry model catalog](https://learn.microsoft.com/azure/search/vector-search-vectorizer-azure-machine-learning-ai-studio-catalog) |\n",
    "\n",
    "Azure Vision supports text and image vectorization.\n",
    "\n",
    "At this time, you can only specify `embed-v-4-0` programmatically through the [AML skill](https://learn.microsoft.com/azure/search/cognitive-search-aml-skill) or [Microsoft Foundry model catalog vectorizer](https://learn.microsoft.com/azure/search/vector-search-vectorizer-azure-machine-learning-ai-studio-catalog), not through the Azure portal. However, you can use the portal to manage the skillset or vectorizer afterward.\n",
    "\n",
    "Deployed models in the model catalog are accessed over an AML endpoint. We use the existing AML skill for this connection.\n",
    "\n",
    "**NOTE:** Inputs to an embedding models are typically chunked data. In an Azure AI Search RAG pattern, chunking is handled in the indexer pipeline, covered in [another notebook](2-build-the-pipeline.ipynb) in this series.\n",
    "\n",
    "### Review models used for generative AI at query time\n",
    "\n",
    "Azure AI Search doesn't have integration code for chat models, so you should choose an LLM that you're familiar with and that meets your requirements. You can modify query code to try different models without having to rebuild an index or rerun any part of the indexing pipeline. Review [Search and generate answers](3-search-and-generate-answers.ipynb) for code that calls the chat model.\n",
    "\n",
    "The following Azure OpenAI models are commonly used for a chat search experience:\n",
    "\n",
    "- GPT-4\n",
    "- GPT-4o\n",
    "- GPT-4.1\n",
    "- GPT-5\n",
    "\n",
    "GPT-4 and GPT-5 models are optimized to work with inputs formatted as a conversation.\n",
    "\n",
    "We use GPT-4o in this exercise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc081cf",
   "metadata": {},
   "source": [
    "## Design an index\n",
    "\n",
    "An index contains searchable text and vector content, plus configurations. In a classic RAG pattern that uses a chat model for responses, you want an index designed around chunks of content that can be passed to an LLM at query time. This section covers the characteristics of an index schema that works for classic RAG.\n",
    "\n",
    "In conversational search, LLMs compose the response that the user sees, not the search engine, so you don't need to think about what fields to show in your search results, and whether the representations of individual search documents are coherent to the user. Depending on the question, the LLM might return verbatim content from your index, or more likely, repackage the content for a better answer.\n",
    "\n",
    "### Organized around chunks\n",
    "\n",
    "When LLMs generate a response, they operate on chunks of content for message inputs, and while they need to know where the chunk came from for citation purposes, what matters most is the quality of message inputs and its relevance to the user's question. Whether the chunks come from one document or a thousand, the LLM ingests the information or *grounding data*, and formulates the response using instructions provided in a system prompt.\n",
    "\n",
    "Chunks are the focus of the schema, and each chunk is the defining element of a search document in a RAG pattern. You can think of your index as a large collection of chunks, as opposed to traditional search documents that probably have more structure, such as fields containing uniform content for a name, descriptions, categories, and addresses.\n",
    "\n",
    "### Enhanced with generated data\n",
    "\n",
    "In this exercise, sample data consists of PDFs and content from the [NASA Earth Book](https://www.nasa.gov/ebooks/earth/). This content is descriptive and informative, with numerous references to geographies, countries, and areas across the world. All of the textual content is captured in chunks, but recurring instances of place names create an opportunity for adding structure to the index. \n",
    "\n",
    "By adding skills, it's possible to recognize entities in the text and capture them in an index for use in queries and filters. We include an [entity recognition skill](https://learn.microsoft.com/azure/search/cognitive-search-skill-entity-recognition-v3) that recognizes and extracts location entities, loading it into a searchable and filterable `locations` field. Adding structured content to your index gives you more options for filtering, improved relevance, and more focused answers.\n",
    "\n",
    "### Parent-child fields in one or two indexes?\n",
    "\n",
    "Chunked content typically derives from a larger document. And although the schema is organized around chunks, you also want to capture properties and content at the parent level. Examples of these properties might include the parent file path, title, authors, publication date, or a summary.\n",
    "\n",
    "An inflection point in schema design is whether to have two indexes for parent and child/chunked content, or a single index that repeats parent elements for each chunk.\n",
    "\n",
    "In this series, because all of the chunks of text originate from a single parent (NASA Earth Book), you don't need a separate index dedicated to up level the parent fields. However, if you're indexing from multiple parent PDFs, you might want a parent-child index pair to capture level-specific fields and then send [lookup queries](https://learn.microsoft.com/rest//rest/api/searchservice/documents/get) to the parent index to retrieve those fields relevant to each chunk.\n",
    "\n",
    "### Checklist of schema considerations\n",
    "\n",
    "In Azure AI Search, an index that works best for RAG workloads has these qualities:\n",
    "\n",
    "- Returns chunks that are relevant to the query and readable to the LLM. LLMs can handle a certain level of dirty data in chunks, such as mark up, redundancy, and incomplete strings. While chunks need to be readable and relevant to the question, they don't need to be pristine.\n",
    "\n",
    "- Maintains a parent-child relationship between chunks of a document and the properties of the parent document, such as the file name, file type, title, author, and so forth. To answer a query, chunks could be pulled from anywhere in the index. Association with the parent document providing the chunk is useful for context, citations, and follow up queries.\n",
    "\n",
    "- Accommodates the queries you want create. You should have fields for vector and hybrid content, and those fields should be attributed to support specific query behaviors, such as searchable or filterable. You can only query one index at a time (no joins) so your fields collection should define all of your searchable content.\n",
    "\n",
    "- Your schema should either be flat (no complex types or structures), or you should [format the complex type output as JSON](https://learn.microsoft.com/azure/search/search-get-started-rag?pivots=csharp#send-a-complex-rag-query) before sending it to the LLM. This requirement is specific to the RAG pattern in Azure AI Search.\n",
    "\n",
    "**NOTE:** Schema design affects storage and costs. This exercise is focused on schema fundamentals. In the [Minimize storage and costs](5-minimize-storage-and-costs.ipynb) exercise, you revisit schemas to learn how narrow data types, compression, and storage options significantly reduce the amount of storage used by vectors.\n",
    "\n",
    "### A minimal index designed for RAG workloads\n",
    "\n",
    "A minimal index for LLM is designed to store chunks of content. It typically includes vector fields if you want similarity search for highly relevant results. It also includes nonvector fields for human-readable inputs to the LLM for conversational search. Nonvector chunked content in the search results becomes the grounding data sent to the LLM.\n",
    "\n",
    "Here's a minimal index definition for RAG solutions that support vector and hybrid search. Review it for an introduction to required elements: index name, fields, and a configuration section for vector fields.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"name\": \"example-minimal-index\",\n",
    "    \"fields\": [\n",
    "    { \"name\": \"id\", \"type\": \"Edm.String\", \"key\": true },\n",
    "    { \"name\": \"chunked_content\", \"type\": \"Edm.String\", \"searchable\": true, \"retrievable\": true },\n",
    "    { \"name\": \"chunked_content_vectorized\", \"type\": \"Edm.Single\", \"dimensions\": 1536, \"vectorSearchProfile\": \"my-vector-profile\", \"searchable\": true, \"retrievable\": false, \"stored\": false },\n",
    "    { \"name\": \"metadata\", \"type\": \"Edm.String\", \"retrievable\": true, \"searchable\": true, \"filterable\": true }\n",
    "    ],\n",
    "    \"vectorSearch\": {\n",
    "        \"algorithms\": [\n",
    "            { \"name\": \"my-algo-config\", \"kind\": \"hnsw\", \"hnswParameters\": { }  }\n",
    "        ],\n",
    "        \"profiles\": [ \n",
    "        { \"name\": \"my-vector-profile\", \"algorithm\": \"my-algo-config\" }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Fields must include key field (`\"id\"` in this example) and should include vector chunks for similarity search, and nonvector chunks for inputs to the LLM. \n",
    "\n",
    "Vector fields are associated with algorithms that determine the search paths at query time. The index has a vectorSearch section for specifying multiple algorithm configurations. Vector fields also have [specific types](https://learn.microsoft.com/rest/api/searchservice/supported-data-types#edm-data-types-for-vector-fields) and extra attributes for embedding model dimensions. `Edm.Single` is a data type that works for commonly used LLMs. For more information about vector fields, see [Create a vector index](https://learn.microsoft.com/azure/search/vector-search-how-to-create-index?tabs=push%2Cportal-check-index).\n",
    "\n",
    "Metadata fields might be the parent file path, creation date, or content type and are useful for [filters](https://learn.microsoft.com/azure/search/vector-search-filters?tabs=prefilter-mode).\n",
    "\n",
    "### The index schema for this series\n",
    "\n",
    "Here's the index schema for the [Earth Book content](https://github.com/Azure-Samples/azure-search-sample-data/tree/main/nasa-e-book/earth_book_2019_text_pages) used in this series.\n",
    "\n",
    "Like the basic schema, it's organized around chunks. The `chunk_id` uniquely identifies each chunk. The `text_vector` field is an embedding of the chunk. The nonvector `chunk` field is a readable string. The `title` maps to a unique metadata storage path for the blobs. The `parent_id` is the only parent-level field, and it's a base64-encoded version of the parent file URI. \n",
    "\n",
    "In integrated vectorization workloads like the one used in this series, the `dimensions` property on your vector fields should be identical to the number of `dimensions` generated by the embedding skill used to vectorize your data. In this series, we use the Azure OpenAI embedding skill, which calls the text-embedding-3-large model on Azure OpenAI. The skill is specified in the next exercise. We set dimensions to 1024 in both the vector field and in the skill definition.\n",
    "\n",
    "The schema also includes a `locations` field for storing generated content that's created by the [indexing pipeline](2-build-the-pipeline.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59d89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.identity import get_bearer_token_provider\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    SearchIndex\n",
    ")\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Create a search index  \n",
    "index_name = \"py-rag-tutorial-idx\"\n",
    "index_client = SearchIndexClient(endpoint=AZURE_SEARCH_SERVICE, credential=credential)  \n",
    "fields = [\n",
    "    SearchField(name=\"parent_id\", type=SearchFieldDataType.String),  \n",
    "    SearchField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"locations\", type=SearchFieldDataType.Collection(SearchFieldDataType.String), filterable=True),\n",
    "    SearchField(name=\"chunk_id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True, analyzer_name=\"keyword\"),  \n",
    "    SearchField(name=\"chunk\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False),  \n",
    "    SearchField(name=\"text_vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), vector_search_dimensions=1024, vector_search_profile_name=\"myHnswProfile\")\n",
    "    ]  \n",
    "    \n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(  \n",
    "    algorithms=[  \n",
    "        HnswAlgorithmConfiguration(name=\"myHnsw\"),\n",
    "    ],  \n",
    "    profiles=[  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"myHnswProfile\",  \n",
    "            algorithm_configuration_name=\"myHnsw\",  \n",
    "            vectorizer_name=\"myOpenAI\",  \n",
    "        )\n",
    "    ],  \n",
    "    vectorizers=[  \n",
    "        AzureOpenAIVectorizer(  \n",
    "            vectorizer_name=\"myOpenAI\",  \n",
    "            kind=\"azureOpenAI\",  \n",
    "            parameters=AzureOpenAIVectorizerParameters(  \n",
    "                resource_url=AZURE_OPENAI_ACCOUNT,  \n",
    "                deployment_name=\"text-embedding-3-large\",\n",
    "                model_name=\"text-embedding-3-large\"\n",
    "            ),\n",
    "        ),  \n",
    "    ], \n",
    ")  \n",
    "    \n",
    "# Create the search index\n",
    "index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)  \n",
    "result = index_client.create_or_update_index(index)  \n",
    "print(f\"{result.name} created\")  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
